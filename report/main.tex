\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[
    backend=biber,
    style=apa,
    citestyle=authoryear
]{biblatex}
\setlength{\bibitemsep}{0.5em}
\addbibresource{references.bib}
\usepackage{parskip}
\usepackage{xcolor}

\title{Multi-core Synergy: A Study of performance improvements utilizing multi-core threading in python}
\author{J. Wolffrom}
\date{\today}

\begin{document} 

\maketitle
\newpage

\begin{abstract}

\end{abstract}
\newpage

\tableofcontents

\newpage
\section{Introduction}

According to Moore's Law, the number of transistors on a microchip doubles every two years, leading to an exponential increase in computing power. 
However, this trend has slowed down in recent years, as the physical limitations of silicon-based technology have been reached \parencite{Mattson2014}.

\begin{quote}
    \textit{\textcolor{darkgray}{Increased clock speeds meant higher heat generation and power consumption, 
    necessitating more robust cooling solutions and improved power management techniques. 
    The quest for ever-higher clock speeds eventually reached a plateau due to these limitations, leading to a shift in CPU design principles.}} 
    - \parencite{mscodes} 
\end{quote}

As a result, the focus has shifted from increasing clock speeds to adding more cores to a processor. This has led to the rise of multi-core processors, which contain multiple processing units on a single chip. 
These processors can execute multiple orders simultaneiously, which can lead to significant performance improvements in software applications, however, a discrepancy can be found in the communities that rely on performance improvements.

\begin{quote}
    \textit{\textcolor{darkgray}{While there are some PC games that love CPUs with a dozen or more cores, theyâ€™re few and far between. 
    Instead, finding an 8-core, 16-thread processor with a high clock speed and a lot of L3 Cache is going to get you further than just adding more CPU cores to the equation. }} 
    - \parencite{Thomas2025}
\end{quote}

Within the gaming communities we're seeing a trend of diminishing returns, whereas what we should be seeing is an equivalance. There is a concensus that multi-core threading is the future of software development, and it has been for several years now, even as far back as 2005 \parencite{mscodes}.

\subsection{The Problem Area}

The question then remains, why are we not seeing the performance improvements that we should be seeing?

If multi-core threading is the future, then it is essential to understand why it is not delivering the expected performance improvements it has the potential to deliver. 
As such, this study aims to address this question by investigating the challenges of multi-core threading and identifying potential solutions to improve performance.

\subsubsection{Research Question}

This being said, there are limits to what can be achieved by the short span of this study, and as such we will be focusing on implementing a solution in python, 
as it is a language that is widely used and has a large community of developers.

The research question that we will be addressing is then as follows:

\begin{quote}
    \textit{How can we improve the performance of software applications by utilizing multi-core threading in Python?}
\end{quote}

This question can then be further broken down into the following sub-questions:

\begin{itemize}
    \item What is the basic makeup of a cpu?
    \item What is a thread, and how can it be accessed in python?
    \item What frameworks are commonly used for multi-core threading, and how can they be extended to use in python?
    \item What are the challenges of multi-core threading, and how can they be overcome?
\end{itemize}

These questions serve as a guide for the study and will help to structure the research process, beginning with a review of the literature on the topic.

\section{Methodology}

Overall introduction to the methodology, and how it will be structured.

\subsection{CPU Architecture}

The difference between performace cores and "optional" cores

\subsubsection{Instruction Pipeline}
The execution of each instruction is partitioned into several steps that are performed by dedicated hardware units one after another


These hardware units are also called pipeline stages

Overlaps multiple instructions

Is similar to an assembly line

Typical numbers of pipeline stages lie inbetween 2 and 26 stages

\subsubsection{Clock Frequency}

The number of clock cycles per second

Also called clock speed

Meazured in hertz which is 1/second(abbreviated as Hz)

\subsubsection{Clock Cycle Time}

The clock frequency F determines the clock cycle time T of the processor

Formula: T = 1/F

Usually the time needed for the execution of one instruction

\subsubsection{Parallel execution time}

Consists of the time it takes for all cores or processors to finish a given program

Also the time for data exchange or synchronization

Should be smaller than the time for a synchronized single core process to be worth it

Influenced by idle times

Smallest execution time occurs generally when workload is distributed equally amongst cores/processors

Speedup and Efficiency is used to quantitatively measure the parallel execution time

\subsubsection{Load Balancing}

When the workload of a program is distributed equally amongst the machine's cores or processors

\subsubsection{Idle Times}

The time a processor cannot do anything useful but wait for more work

\subsubsection{Barrier Operations}

Barrier points where both distributed and shared memory machines have to wait for all cores to synchronize

\subsubsection{Memory}

Memory distribution methods per multi-core systems

Can use bits of both

\subsubsection{Shared Memory}

Memory organization where the machine shares memory for all threads

Synchronization plays a heavy role, for example by keeping threads from reading files before another has written to them

Often connected to the term "Thread"

\subsubsection{Distributed Memory}

Memory organization where the machine distributes memory per processor

Communication happens explicitly via communication operations

Often connected to the term "Process"

\subsubsection{Core}

Refers to single computing units

\subsubsection{Multicore}

Refers to entire processor having several cores

\subsubsection{Tasks}

Bits of instructions broken down into something more that can be managed on multiple cores

Are assigned to processes or threads which are then assigned to physical computation units for execution

\subsubsection{Synchronization}

Synchronization and coordination of threads and processes in order to execute them correctly

Strongly connected with the way in which information is exchanged between processes or threads

Hardware dependant as well

\subsubsection{Scheduling}

The process of assigning tasks to processes or threads

Fixes the order in which the tasks are executed

Can be done by hand in the source code or by the programming environment, at compile time or dynamically at runtime

\subsubsection{Mapping}

The assignment of processes or threads onto the physical units, processors or cores

Usually done by the runtime system but can sometimes be influenced by the programmer

\subsubsection{Decomposition}

Decomposing computations down into several tasks for multiple cores to process in parallel

There are many different types of decomposition

\subsubsection{Granularity}

The size of tasks in terms of the number of instructions

\subsubsection{Potential Parallelism}

An inherited property of an application algorithm

Influences how an application can be split into tasks

\subsubsection{ILP}

Instruction Level Parallelism (ILP)

Processors which use pipelining to execute instructions are ILP processors

\subsubsection{Throughput}

The number of instructions finished per unit time of a pipeline

In the absence of dependencies, the throughput is one instruction per clock cycle and the pipelines all work in parallel

The absence of dependencies of pipelines determine the degree of parallelism attainable

\subsubsection{Superpipelined}

Processors with a relatively large number of pipeline stages

\subsubsection{ALU}

Arithmetic Logical Unit

\subsubsection{FPU}

Floating Point Unit 

\subsubsection{Flynn's Taxonomy of Parallel Architectures}

\subsubsection{SIMD}

Single Instruction Multiple Data

\subsubsection{MIMD}

Multiple Instruction Multiple Data

\subsubsection{Superscalar Processors}

Superscalar processors have their parallel dependencies determined dynamically at runtime by hardware

Decoded instructions are dispatched to the instruction units by hardware using dynamic Scheduling

Makes the circut complexity increase significantly

Simulations show that superscalar processors with up to four functional units yield substantial benefits over the single

\subsubsection{VLIW Processors}

Very Long Instruction Word (VLIW)

\subsubsection{Computing Systems}

Comprises all the hardware and software components which are provided to the programmer

Forms the programmer's view of the machine

\subsubsection{Software Aspects}

Operating System

Programming Language

Compiler or Runtime Libraries

\subsubsection{Hardware Aspects}

Processor Architecture

\subsubsection{Four Parallel Processing Models}

\subsubsection{Machine Model}

Lowest level of abstraction

Describes the hardware and operating system

\subsubsection{Architectural Model}

Interconnection network of parallel platforms

Memory organization

Synchronous or Asynchronous processing

Execution mode of single instructions by SIMD or MIMD

\subsubsection{Computational Model}

\subsubsection{Programming Model}

\subsubsection{Threading}

Threading and multi-core utilization 

\subsection{Python}

If there turns out to be something that limits the performance of python in multi-core threading this might be useful to talk about.

\subsection{Concurrency and Parallelism}

Presentation on the two paradigms, and how they both are used in multi-core threading.

\subsubsection{Asynchronous Programming}

Short subsection on async, and how it uses a single thread to run multiple tasks using methods to suspend itself whilst awaiting changes.

\subsubsection{Subroutines and Coroutines}

Small introduction to Subroutines and Coroutines as methods, and liking them directly to "def" in python.

Inclue something about time.sleep in python, it syspends a thread just like async.

\section{Data Collection}

Section on how the data was collected, what was collected and the code used. 

Potentially rename the section to something more fitting.

\section{Results}

\section{Discussion}

Include thoughts about other programming languages.

\section{Conclusion}
\newpage
\section{References}
\printbibliography

\end{document}